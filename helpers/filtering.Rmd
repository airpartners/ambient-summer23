---
title: "Filtering"
author: "Cherry Pham"
date: "2023-07-25"
output: html_document
---

```{r setup, include=FALSE}
# Check and install required packages if necessary
packages <- c(
  "openair",
  "ggplot2",
  "dplyr",
  "pracma",
  "lubridate"
)
install.packages(packages[!sapply(packages, requireNamespace, quietly = TRUE)])

# Load required packages for data manipulation and analysis
invisible(sapply(packages, library, character.only = TRUE))

# Set options
knitr::opts_chunk$set(echo = FALSE, message = FALSE)

# Suppress Warnings (This is a me thing, comment this session out to see the conflicting functions)
suppressPackageStartupMessages(library(maps))
```

```{r}
# Load data frame
load("../data/graphableData.RData")
```

### PM10

```{r}
# Create sub-df to reduce temp memory usage
mod_met_pm10 <- mod_met[, c("date", "sn", "pm10", "ws", "wd")]

# Filter the data to include only the desired time range
Sys.setenv(TZ = "America/New_York")
start_date <- as.Date("2022-12-23")
end_date <- as.Date("2022-12-24")
mod_met_filtered <- mod_met_pm10[mod_met_pm10$date >= start_date & mod_met_pm10$date <= end_date, ]

print(mod_met_filtered)
```

```{r}
# Get a vector of unique sensors in the dataframe
sensors <- unique(mod_met_filtered$sn)

# # Loop through each sensor and generate the plots
for (sensor in sensors) {
  sensor_data <- subset(mod_met_filtered, sn == sensor)

  # Convert the date column to a POSIXct object (if it's not already)
  sensor_data$date <- as.POSIXct(sensor_data$date)

  p <- timePlot(
    sensor_data,
    pollutant = "pm10",
    ylab = "pm10 (µg/m³)",
    main = paste("Sensor:", sensor),
    avg.time = "hour",
    y.relation = "same",
    ylim = c(0, 150),
    windflow = list(col = "grey", lwd= 2, scale = 0.1)
  )
}
```

```{r}
# Step 1: Create a dataset pm10_hourly that averages data in mod_met_filtered by the hour
pm10_hourly <- mod_met_filtered %>%
  mutate(date = as.POSIXct(date)) %>%
  group_by(sn, hour = lubridate::floor_date(date, unit = "hour")) %>%
  summarise(pm10_avg = mean(pm10, na.rm = TRUE))
pm10_hourly <- pm10_hourly %>%
  filter(!is.nan(pm10_avg))

# Step 2: Find peaks in pm10_hourly$pm10 for each sensor and save the result into an array
peaks_list <- list()
unique_sensors <- unique(pm10_hourly$sn)
for (sensor in unique_sensors) {
  sensor_data <- subset(pm10_hourly, sn == sensor)
  peaks <- findpeaks(sensor_data$pm10_avg, threshold = 50)
  peaks_list[[paste0("peaks_", sensor)]] <- peaks[, 1:2]
}

# Step 3: Traverse pm10_hourly by sensor and create histogram_data
histogram_data <- data.frame(date = character(), count = integer())

for (sensor in unique_sensors) {
  peaks_array_name <- paste0("peaks_", sensor)
  peaks <- peaks_list[[peaks_array_name]]

  pm10_data <- subset(pm10_hourly, sn == sensor)
  peak_dates <- pm10_data$hour[peaks[, 2]]
  peak_dates <- as.Date(peak_dates)
  peak_dates <- na.omit(peak_dates)

  for (peak_date in peak_dates) {
    if (peak_date %in% histogram_data$date) {
      histogram_data$count[histogram_data$date == peak_date] <-
        histogram_data$count[histogram_data$date == peak_date] + 1
    } else {
      histogram_data <-
        rbind(histogram_data, data.frame(date = peak_date, count = 1))
    }
  }
}

# Filter histogram_data to only include counts greater than 1
histogram_data <- histogram_data %>%
  filter(count > 1)

# Convert date to "YYYY-MM-DD" format in a new column
histogram_data$date <- as.Date(histogram_data$date)  # Convert to actual date objects

# View the resulting histogram_data
print(histogram_data)

ggplot(histogram_data, aes(x = as.character(date), y = count)) +
  geom_bar(stat = "identity", fill = "#D48FF2", width = 0.7) +
  labs(
    x = "Day",
    y = "Number of Sensors",
    title = "Number of Sensors with PM10 Data Peaks per Day"
  ) +
  scale_y_continuous(breaks = 1:30) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
    text = element_text(family = "Helvetica"))
```
