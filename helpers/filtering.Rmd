---
title: "Filtering"
author: "Cherry Pham"
date: "2023-07-25"
output: html_document
---

```{r setup, include=FALSE}
# Check and install required packages if necessary
packages <- c(
  "openair",
  "ggplot2",
  "dplyr",
  "pracma",
  "lubridate"
)
install.packages(packages[!sapply(packages, requireNamespace, quietly = TRUE)])

# Load required packages for data manipulation and analysis
invisible(sapply(packages, library, character.only = TRUE))

# Set options
knitr::opts_chunk$set(echo = FALSE, message = FALSE)

# Suppress Warnings (This is a me thing, comment this session out to see the conflicting functions)
suppressPackageStartupMessages(library(maps))
```

```{r}
# Load data frame
load("../data/graphableData.RData")
```

### PM10

```{r}
# Create sub-df to reduce temp memory usage
mod_met_pm10 <- mod_met[, c("date", "sn", "pm10", "ws", "wd")]

# Filter the data to include only the desired time range
Sys.setenv(TZ = "America/New_York")
start_date <- as.Date("2022-07-01")
end_date <- as.Date("2023-02-28")
mod_met_filtered <- mod_met_pm10[mod_met_pm10$date >= start_date & mod_met_pm10$date <= end_date, ]

print(mod_met_filtered)
```

```{r}
# Get a vector of unique sensors in the dataframe
sensors <- unique(mod_met_filtered$sn)

# # Loop through each sensor and generate the plots
for (sensor in sensors) {
  sensor_data <- subset(mod_met_filtered, sn == sensor)

  # Convert the date column to a POSIXct object (if it's not already)
  sensor_data$date <- as.POSIXct(sensor_data$date)

  p <- timePlot(
    sensor_data,
    pollutant = "pm10",
    ylab = "pm10 (µg/m³)",
    main = paste("Sensor:", sensor),
    avg.time = "hour",
    y.relation = "same",
    ylim = c(0, 500)
  )
}
```

```{r}
# Step 1: Create a dataset pm10_hourly that averages data in mod_met_filtered by the hour
pm10_hourly <- mod_met_filtered %>%
  mutate(date = as.POSIXct(date)) %>%
  group_by(sn, hour = lubridate::floor_date(date, unit = "hour")) %>%
  summarise(pm10_avg = mean(pm10, na.rm = TRUE))

# Step 2: Remove NaN values
pm10_hourly <- pm10_hourly %>%
  filter(!is.nan(pm10_avg))

# Step 2: Find peaks in pm10_hourly$pm10 for each sensor and save the result into an array
peaks_list <- list()
unique_sensors <- unique(pm10_hourly$sn)
for (sensor in unique_sensors) {
  sensor_data <- subset(pm10_hourly, sn == sensor)
  peaks <- findpeaks(sensor_data$pm10_avg, threshold = 50)
  peaks_list[[paste0("peaks_", sensor)]] <- peaks[, 1:2]
}
# Create an empty data frame to store the histogram data
histogram_data <-
  data.frame(date = as.POSIXct(character()), count = integer())
timestamp_dict <- list()

# Step 4: Loop through pm10_hourly per sensor and update histogram_data
unique_sensors <- unique(pm10_hourly$sn)
# Create an empty data frame to store the results
result_df <- data.frame(timestamp = character(), occurrences = integer(), stringsAsFactors = FALSE)

for (sensor in unique_sensors) {
  print(sensor)
  # Get the corresponding peaks array
  peaks_array <- peaks_list[[paste0("peaks_", sensor)]]
  
  # Get the peak indices
  peak_indices <- peaks_array[, 2]
  
  # Get the dates corresponding to the peak indices
  peak_dates <- pm10_hourly$hour[peak_indices]
  
  # Count the occurrences of each timestamp
  timestamp_counts <- table(peak_dates)
  
  # Append the results to the data frame
  result_df <- rbind(result_df, data.frame(timestamp = names(timestamp_counts), occurrences = as.integer(timestamp_counts)))
}

# Print the results in the desired format
cat("Timestamp: Occurrences\n")
for (i in seq_len(nrow(result_df))) {
  cat(paste(result_df$timestamp[i], ": ", result_df$occurrences[i], "\n", sep = ""))
}

# for (sensor in unique_sensors) {
#   print(sensor)
#   # Get the corresponding peaks array
#   peaks_array <- peaks_list[[paste0("peaks_", sensor)]]
#   
#   # Get the peak indices
#   peak_indices <- peaks_array[, 2]
#   
#   # Get the dates corresponding to the peak indices
#   peak_dates <- pm10_hourly$hour[peak_indices]
#   print(peak_dates)
#   # posixct_array_common <-
#   #   force_tz(peak_dates, tzone = "UTC")
#   # print(posixct_array_common)
#   
#   # Loop through peak_dates
#   for (date in peak_dates) {
#     # print(as.character(date))
#     # Check if the date exists in histogram_data
#     if (!(date %in% histogram_data$date)) {
#       # If the date does not exist, add a new row to histogram_data
#       histogram_data <-
#         rbind(histogram_data, data.frame(date = date, count = 1))
#     } else {
#       # If the date exists, increment the count by 1
#       histogram_data$count[histogram_data$date == date] <-
#         histogram_data$count[histogram_data$date == date] + 1
#     }
#   }
# }
# 
# # Step 5: Sort the histogram_data dataframe by date
# histogram_data <- histogram_data[order(histogram_data$date), ]
# 
# # Step 6: Visualize the resulting histogram_data dataframe
# print(histogram_data)
# 
# # ggplot(histogram_data, aes(x = date_formatted, y = count)) +
# #   geom_bar(stat = "identity", fill = "#D48FF2", width = 0.7) +
# #   labs(
# #     x = "Day",
# #     y = "Number of Sensors",
# #     title = "Number of Sensors with PM10 Data Peaks per Day"
# #   ) +
# #   scale_y_continuous(breaks = 1:10) +
# #   theme_minimal() +
# #   theme(
# #     axis.text.x = element_text(angle = 90, hjust = 1),
# #     text = element_text(family = "Helvetica"))
```
